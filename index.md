---
layout: homepage
---

## Welcome!
I am a second-year Ph.D. student in electrical and computer engineering (ECE) at Seoul National University (SNU), advised by Prof. [Se Young Chun](https://icl.snu.ac.kr/pi).
  I completed my M.S. in Bio and Brain Engineering (BBE)  at Korea Advanced Institute of Science and Technology (KAIST), advised by Prof. [Jong Chul Ye](https://bispl.weebly.com/professor.html).
I earned my B.S degree in electrical and electronic engineering (EEE) from Yonsei University, advised by Prof. [Dong Hyun Kim](http://kimchi.yonsei.ac.kr/default/01/01.php#s1).

## Experience

- **<span style="color:#4285F4">G</span><span style="color:#EA4335">o</span><span style="color:#FBBC05">o</span><span style="color:#4285F4">g</span><span style="color:#34A853">l</span><span style="color:#EA4335">e</span> Research**, Student Researcher
  <br>
  Hosted by [Alonso Martinez](https://www.linkedin.com/in/alonsomartinez/) and [Krishna Somandepalli](https://sail.usc.edu/~somandep/)  (Starting Sep. 2023)
- **Fast Campus**, Lecturer in AI
  <br>
  Letcure title: [Mastering GANs through Model Implementation](https://fastcampus.co.kr/data_online_ganmodel) (Nov. 2022 - Present)


## Research Interests
My research interests lie broadly in the field of computer vision (CV), machine learning (ML). Specifically, my work explores the following areas:
- **Generative models** and their applications (Diffusion models, GANs)
- **Multi-modal learning** (Text-Image, Text-Video, Audio-Video)  
- **3D reconstruction and 3D-aware synthesis** 
- **Privacy-preserving distributed learning** (Federated learning, Split learning)
- **Weakly- and Self-supervised learning**
- 
but not limited to.

## <b style="color:#F88017">News</b>

- **[Sep. 2023]** I am joining [<span style="color:#4285F4">G</span><span style="color:#EA4335">o</span><span style="color:#FBBC05">o</span><span style="color:#4285F4">g</span><span style="color:#34A853">l</span><span style="color:#EA4335">e</span> Research](https://research.google/) as a Student Researcher.
- **[Aug. 2023]** I am selected as a recipient of [Yulchon AI Star Scholarship](https://aiis.snu.ac.kr/bbs/board.php?bo_table=eng4_3). 
- **[Jul. 2023]** [PODIA-3D](https://arxiv.org/pdf/2304.01900.pdf) is accepted to [ICCV 2023](https://iccv2023.thecvf.com/).
- **[Apr. 2023]** [Ditto-NeRF](https://arxiv.org/pdf/2304.02827.pdf) is available on arXiv.
- **[Feb. 2023]** [DATID-3D](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf) is accepted to [CVPR 2023](https://cvpr2023.thecvf.com/).
- **[Nov. 2022]** I start a new postiion as [Lecturer in AI](https://fastcampus.co.kr/data_online_ganmodel) at [Fast Campus](https://fastcampus.co.kr/).
- **[Aug. 2022]** [DiffusionCLIP](https://openaccess.thecvf.com/content/CVPR2022/html/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.html) is invited to oral session at [KCCV 2022](http://kccv2022.kcvs.kr/).
- **[Jun. 2022]** [DISTL](https://www.nature.com/articles/s41467-022-31514-x) is accepted to [Nature Communications](https://www.nature.com/ncomms/aims?gclid=Cj0KCQjwzLCVBhD3ARIsAPKYTcRgreQTZQgfNQoa9T20b_DFX47TpljFwkD09uQTo00ca6hwEV4eqcEaAh1_EALw_wcB).
- **[Mar. 2022]** [DiffusionCLIP](https://openaccess.thecvf.com/content/CVPR2022/html/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.html) is accepted to [CVPR 2022](https://cvpr2022.thecvf.com/).
- **[Dec. 2021]** I won the [Best Master Student for the Year Award](https://bispl.weebly.com/bispl-hall-of-fame) at [BISPL](https://bispl.weebly.com/), KAIST.
- **[Dec. 2021]** Our [AI-based COVID-19 diagnosis solution](https://www.promedius.ai/product/1) passed exploratory clinical trial.
- **[Nov. 2021]** [MT-ViT for COVID-19](https://www.sciencedirect.com/science/article/pii/S1361841521003443) is accepted to [Medical Image Analysis](https://www.journals.elsevier.com/medical-image-analysis).
- **[Sep. 2021]** [FeSTA](https://papers.nips.cc/paper/2021/file/ceb0595112db2513b9325a85761b7310-Paper.pdf) is accepted to [NeurIPS 2021](https://neurips.cc/Conferences/2021).
- **[Dec. 2020]** We won the 2nd place award in [NVIDIA AI Healthcare Hackathon](https://dreamai.kr/fair_nvidia).

## Research

<div class="publications">
<ol class="bibliography">


<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/podia_3d.gif" class="teaser img-fluid z-depth-1">
    <abbr class="badge">ICCV</abbr>
  </div>
  <div id="podia_3d" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2304.01900.pdf">PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion</a></div>
      <div class="author"><strong>Gwanghyun Kim</strong>, J. H. Jang, S. Y. Chun </div>
      <div class="periodical"><em><strong>ICCV 2023</strong></em>
      </div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2304.01900.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://gwang-kim.github.io/podia_3d/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project page</a>
      <a href="https://github.com/gwang-kim/PODIA-3D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://github.com/gwang-kim/PODIA-3D" target="_blank" rel="noopener"><strong><i style="color:#e74d3c; font-weight:600" id="githubstars_manets5"></i><i style="color:#e74d3c; font-weight:600"> GitHub Stars</i></strong></a>
    <script>
    githubStars("gwang-kim/PODIA-3D", function(stars) {
    var startext = document.getElementById("githubstars_manets5");
          startext.innerHTML=stars;
    });
    </script>
    </div>
  </div>
</div>
</li>



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/ditto_nerf.gif" class="teaser img-fluid z-depth-1">
  </div>
  <div id="ditto_nerf" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2304.02827.pdf">DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model </a></div>
      <div class="author">H. Seo*, H. Kim*, <strong>Gwanghyun Kim*</strong>, S. Y. Chun <strong>(*co-first)</strong> </div>
      <div class="periodical"><em><strong>arXiv, 2023</strong></em>
      </div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2304.02827.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://janeyeon.github.io/ditto-nerf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project page</a>
      <a href="https://github.com/janeyeon/ditto-nerf-code" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://github.com/janeyeon/ditto-nerf-code" target="_blank" rel="noopener"><strong><i style="color:#e74d3c; font-weight:600" id="githubstars_manets3"></i><i style="color:#e74d3c; font-weight:600"> GitHub Stars</i></strong></a>
    <script>
    githubStars("janeyeon/ditto-nerf-code", function(stars) {
    var startext = document.getElementById("githubstars_manets3");
          startext.innerHTML=stars;
    });
    </script>
    </div>
  </div>
</div>
</li>


<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/datid_3d.gif" class="teaser img-fluid z-depth-1">
    <abbr class="badge">CVPR</abbr>
  </div>
  <div id="datid_3d" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf">DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model</a></div>
      <div class="author"><strong>Gwanghyun Kim</strong>, S. Y. Chun </div>
      <div class="periodical"><em><strong>CVPR 2023</strong></em>
      </div>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://gwang-kim.github.io/datid_3d/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project page</a>
      <a href="https://github.com/gwang-kim/DATID-3D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://huggingface.co/spaces/gwang-kim/DATID-3D" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Demo</a>
      <a href="https://colab.research.google.com/drive/1e9NSVB7x_hjz-nr4K0jO4rfTXILnNGtA?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Colab</a>
      <a href="https://github.com/gwang-kim/DATID-3D" target="_blank" rel="noopener"><strong><i style="color:#e74d3c; font-weight:600" id="githubstars_manets2"></i><i style="color:#e74d3c; font-weight:600"> GitHub Stars</i></strong></a>
    <script>
    githubStars("gwang-kim/DATID-3D", function(stars) {
    var startext = document.getElementById("githubstars_manets2");
          startext.innerHTML=stars;
    });
    </script>
    </div>
  </div>
</div>
</li>



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/self_evolving.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">Nat. Commun.</abbr>
  </div>
  <div id="mt_vit" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://www.nature.com/articles/s41467-022-31514-x">AI can evolve without labels: self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation</a></div>
      <div class="author">S. Park, <strong>Gwanghyun Kim</strong>, Y. Oh, J. B. Seo, S. M. Lee, J. H. Kim, S. Moon, J. K. Lim,  C. M. Park, J. C. Ye </div>
      <div class="periodical"><em><strong>Nature Communications, 2022 </strong></em>
      </div>
    <div class="links">
      <a href="https://www.nature.com/articles/s41467-022-31514-x" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>
</li>


<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/diffusionclip.png" class="teaser img-fluid z-depth-1">
            <abbr class="badge">CVPR</abbr>
  </div>
  <div id="diffusionclip" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.pdf">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation</a></div>
      <div class="author"><strong>Gwanghyun Kim</strong>, T. Kwon, J. C. Ye </div>
      <div class="periodical"><em><strong>CVPR 2022, KCCV 2022 (Oral)</strong></em>
      </div>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/gwang-kim/DiffusionCLIP" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://replicate.com/gwang-kim/diffusionclip" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Demo</a>
      <a href="https://colab.research.google.com/drive/1E8QHZ3BbkF6hzk0rRKzhfkySmYf_BZaE?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Colab</a>
      <a href="https://youtu.be/YVCtaXw6fw8" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Video</a>
      <a href="https://drive.google.com/file/d/1QgRFIRba492dCZ6v7BcZB9zqyp91aTjL/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Poster</a>
      <a href="https://github.com/gwang-kim/DiffusionCLIP" target="_blank" rel="noopener"><strong><i style="color:#e74d3c; font-weight:600" id="githubstars_manets"></i><i style="color:#e74d3c; font-weight:600"> GitHub Stars</i></strong></a>
  <script>
  githubStars("gwang-kim/DiffusionCLIP", function(stars) {
  var startext = document.getElementById("githubstars_manets");
        startext.innerHTML=stars;
  });
  </script>
    </div>
  </div>
</div>
</li>

<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/festa.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">NeurIPS</abbr>
  </div>
  <div id="festa" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://papers.nips.cc/paper/2021/file/ceb0595112db2513b9325a85761b7310-Paper.pdf">Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training</a></div>
      <div class="author">S. Park*, <strong>Gwanghyun Kim*</strong>, J. Kim, B. Kim, J. C. Ye <strong>(*co-first)</strong> </div>
      <div class="periodical"><em><strong>NeurIPS 2021 </strong></em>
      </div>
    <div class="links">
      <a href="https://papers.nips.cc/paper/2021/file/ceb0595112db2513b9325a85761b7310-Paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>
</li>



<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/mt_vit.png" class="teaser img-fluid z-depth-1">
    <abbr class="badge">Med. Image Anal.</abbr>
  </div>
  <div id="mt_vit" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://www.sciencedirect.com/science/article/pii/S1361841521003443">Multi-task Vision Transformer using Low-level Chest X-ray Feature Corpus for COVID-19 Diagnosis and Severity Quantification</a></div>
      <div class="author">S. Park*, <strong>Gwanghyun Kim*</strong>, Y. Oh, J. B. Seo, S. M. Lee, J. H. Kim, S. Moon, J. K. Lim, J. C. Ye <strong>(*co-first)</strong> </div>
      <div class="periodical"><em><strong>Medical Image Analysis, 2021 </strong></em>
      </div>
    <div class="links">
      <a href="https://www.sciencedirect.com/science/article/pii/S1361841521003443" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>
</li>


<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/vit_sev.png" class="teaser img-fluid z-depth-1">
  </div>
  <div id="vit_sev" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2103.07062.pdf">Severity Quantification and Lesion Localization of COVID-19 on CXR using Vision Transformer</a></div>
      <div class="author"><strong>Gwanghyun Kim</strong>, S. Park, Y. Oh, J. B. Seo, S. M. Lee, J. H. Kim, S. Moon, J. K. Lim, J. C. Ye </div>
      <div class="periodical"><em><strong>arXiv, 2021</strong></em>
      </div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2103.07062.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>
</li>


<li>
<div class="pub-row">
  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="./assets/research/vit_covid.png" class="teaser img-fluid z-depth-1">
  </div>
  <div id="vit_covid" class="col-sm-9" style="position: relative;width: 100%;padding-right: 15px;padding-left: 20px;">
      <div class="title"><a href="https://arxiv.org/pdf/2103.07055.pdf">Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature Corpus</a></div>
      <div class="author">S. Park, <strong>Gwanghyun Kim</strong>, Y. Oh, J. B. Seo, S. M. Lee, J. H. Kim, S. Moon, J. K. Lim, J. C. Ye </div>
      <div class="periodical"><em><strong>arXiv, 2021</strong></em>
      </div>
    <div class="links">
      <a href="https://arxiv.org/pdf/2103.07055.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
    </div>
  </div>
</div>
</li>


</ol>
</div>


## Patents

- **Severity quantification and lesion localization method of infectious disease on cxr using vision transformer and apparatus therefor**
  <br>
  J. C. Ye, S. Park, **Gwanghyun Kim**
  <br>
  U.S. Patent Application, No. 17/704,879, 2022

- **Method of classifying lesion of chest x-ray radiograph based on data normalization and local patch and apparatus thereof**
  <br>
  J. C. Ye, S. Park, Y. Oh, **Gwanghyun Kim**
  <br>
  U.S. Patent Application, No. 17/352,229, 2022



## Awards and Honors

- **[Yulchon AI Star Scholarship](https://aiis.snu.ac.kr/bbs/board.php?bo_table=eng4_3) ($8,000)**, Youlchon Foundation & SNU AI Institute, 2023
- **Brain Korea 21 Scholarships**, Korea Research Foundation, 2022 - 2023
- **[Best Master Student for the Year Award](https://bispl.weebly.com/bispl-hall-of-fame) ($2,000)**, BISPL, KAIST, 2021 
- **[2nd Place Award in NVIDIA AI Healthcare Hackathon](https://dreamai.kr/fair_nvidia) ($8,500)**, NVIDIA & GIAI, 2020
- **KAIST Scholarship**, KAIST, 2020 - 2021
- **Honors Scholarship**, Yonsei University, Spring 2018, Fall 2018
  
## Invited Talks

- **Text-driven Control of 2D/3D Image Using Diffusion:
DiffusionCLIP and DATID-3D**
  <br>
  Innerverz Seminar, Innerverz (Remote), 2023
- **DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation**
  <br>
  London Machine Learning Meetup, London Machine Learning Group (Remote), 2022
- **Diffusion Models for Vision-Language Tasks**
  <br>
  Kakao Brain Open Seminar, Kakao Brain, 2022
- **Deep Learning based Diagnosis of Infectious Diseases on CXR and Audio data**
  <br>
  NVIDIA AI Developer Meetup, NVIDIA (Remote), 2020



## Services
- **Conference reviewers:** CVPR 2023, ICCV 2023, NeurIPS 2023
- **Journal Reviewers:** T-PAMI, ACM Comput Surv


[//]: # (## Projects)

[//]: # ()
[//]: # (- **Development of AI Modules for Smart X-ray Screening Systems**)

[//]: # (  <br>)

[//]: # (  Conducted by Korea Customs Service, 2021 - 2021)

[//]: # (  <br>)

[//]: # (  Algorithm development)

[//]: # ()
[//]: # (- **AI Chest X-ray Rapid Diagnosis**)

[//]: # (  <br>)

[//]: # (  Conducted by Korea Aid for Respiratory Epidemic, 2020 - 2021)

[//]: # (  <br>)

[//]: # (  Algorithm development, System deployment, Clinical trial preparation)


